{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kirubanath/ML-DS_learnings/blob/main/Kaggle_comp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBq2PvoWF_F5",
        "outputId": "0bf43696-03d6-4f12-80be-942ab04a5450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U --no-cache-dir gdown "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3RlH8mm6xpY"
      },
      "source": [
        "# Documentation: \n",
        "\n",
        "(I included all the steps I did and all my learnings in the documentation rather than showing everything in the code since it will just reduce the readability of the document)\n",
        "\n",
        "**Loading the data:**\n",
        "1. Surprisingly, loading the data to google colab was a mojor learning. This is because the data itself was very big and my ususla way of doing things failed.\n",
        "\n",
        "**Preprocessing/EDA:**\n",
        "1. I first perfromed the usual steps of seeing the distribution of each column (data.hist()). The data itself looked very well cleaned and there were no missing values/ outliers. \n",
        "\n",
        "2. As the second step I plotted the correlation matrix of the input columns as a heatmap to understand the relationships between the columns. There were few columns that had high correlations, especially since the columns form 9-19 were derivative of first eight columns. This gave me the idea of using just these 10 columns in my model. Infact this actually produced comparable accuracy to using all the features (accuracy just reduced by 0.5%) and even the training time was much faster. In a practical application I will use only these 10 features to train my model since its more efficient, but for the sake of the competition where every bit matters I ended up choosing the entire dataset.\n",
        "\n",
        "3. For the next step I tried reducing the training data itself and rather than using the entire set I only used 10% of the dataset to train my models. Surprisingly this aslo gave comparable accuracy (accuracy reduced by around 0.6%) with much faster training time. Again in real world cases where efficiency is more important than half percent accuracy, I would use this technique rather than using the entire dataset.\n",
        "\n",
        "4. Next I tried adding noise to the input data by randomly selecting 25% of the column data and adding the column mean to it, to make the model more generalized. But again the end accuracy was slightly lower than when I did not use this technique. So I dropped this as well.\n",
        "\n",
        "5. I tried standardizing both the train and test data togather and because of the data leakage, I actually got a slightly better accuracy than the base model. But again for the final submission, I did not use this method since it is not the best practice when it comes to real world applications.\n",
        "\n",
        "6. I also observed that the some columns are very skewed and for the case of linear models like logistic regression it is important to make then as normal before fitting the data. So i tried various transformations like log transform for the right skewed columns and even powertransforms like boxcox for the entire columns. But again I only ended up getting lesser accuracy(even though in theory it should work better. This might be becasue of my limitations and maybe I am missing something here.) In the end had to drop this idea as well.\n",
        "\n",
        "7. As a final step I noticed that the columns had lot of outliers when I plottted the boxplots for each of the columns. Using the winsorize method(I tried both the Z-score and IQR method first but this one gave better results) I capped the outliers and tried again. It did give improvement to the linear models but still at the end since I ended up choosing the boosting classifiers that use decision trees, I did not use that for the final submisison since removing outliers won't make much difference for decision trees.\n",
        "\n",
        "\n",
        "**Model Selection: Default parameters:**\n",
        "1. SVM: I couldn't check with svm models because it was taking a long time and and my machine kept getting interrupted.\n",
        "\n",
        "2. Decision Trees: The accuracy from decision trees were the lowest at around 78%.\n",
        "\n",
        "3. Logistic Regression: Similar to decision trees, the accuracy was very low at 78% \n",
        "\n",
        "4. Random Forest: The model accuracy was around 79%. So with hyperparameter tuning it may be possible to imporve the model.\n",
        "\n",
        "5. Adaboost: All the boosting classifiers performed better than other models. Same goes for Adaboost. It was at around 79%.\n",
        "\n",
        "6. XGBoost: It was one of the two best performing models with accuracy at 80%. This was seelcted for hyperparamter tuning.\n",
        "\n",
        "7. HistGradientBoost: Along with XGboost, it was one of the two best performing models and was selected for hyperparamter tuning.\n",
        "\n",
        "**Model Selection: Hyperparameter tuning:**\n",
        "1. Random Forest: I tried hyperparameter tuning with GridSearchCv but again like in the case of SVM it was taking too long. Then I tried with RandomSearchCV, I did get some results but the accuracy was not increasing much and lingered around 79%. So I abandoned it, but Its just an intuition that maybe with proper hyperparamter random forest would have performed better. But again due to time constraint had to abandon that route.\n",
        "\n",
        "2. XGboost: Just like in the case of Random Forest, exhaustive search was not viable, so used RandomsearchCV with CV =5 and noted the best paramters among the many iterations that I have done. I used those best paramters that I personally could find for this final submission.\n",
        "\n",
        "3. HistGradientBoost: Again did many iterations of RandomSearch and used the best parameters for the final submission. Infact my best model was this one with the tuned parameters. I used this model for the final output submission to the competition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QkjEqs81LVj"
      },
      "outputs": [],
      "source": [
        "#importing necessary libraries for the final submission:\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import random\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "QshS6UqnGLxt",
        "outputId": "0b1214bf-eacf-4609-c84f-9d5af52ec926"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_FZIrNug8svcEK5QMDdV2MoHmcCny8kJ&export=download\n",
            "To: /content/train.csv\n",
            "100%|██████████| 1.24G/1.24G [00:07<00:00, 161MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_FAidr3sAiSWx6mPS26HMpWjVaiSTOXq&export=download\n",
            "To: /content/test.csv\n",
            "100%|██████████| 524M/524M [00:04<00:00, 111MB/s] \n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'test.csv'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = \"https://drive.google.com/uc?id=1_FZIrNug8svcEK5QMDdV2MoHmcCny8kJ&export=download\"\n",
        "output = \"train.csv\"\n",
        "gdown.download(url, output)\n",
        "\n",
        "url = \"https://drive.google.com/uc?id=1_FAidr3sAiSWx6mPS26HMpWjVaiSTOXq&export=download\"\n",
        "output = \"test.csv\"\n",
        "gdown.download(url, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ha60kaJGPz4"
      },
      "outputs": [],
      "source": [
        "data_train  = pd.read_csv('/content/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJCnlqDTGoEE"
      },
      "outputs": [],
      "source": [
        "data_test  = pd.read_csv('/content/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CryLfTxjGsml",
        "outputId": "be2bef26-b372-451a-81a4-1b1fa8feeb12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3500000, 20)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWIuPF1wIu0Z",
        "outputId": "ab44361c-c139-432b-e199-827b3d1179ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3500000 entries, 0 to 3499999\n",
            "Data columns (total 20 columns):\n",
            " #   Column                    Dtype  \n",
            "---  ------                    -----  \n",
            " 0   Unnamed: 0                int64  \n",
            " 1   lepton_1_pT               float64\n",
            " 2   lepton_1_eta              float64\n",
            " 3   lepton_1_phi              float64\n",
            " 4   lepton_2_pT               float64\n",
            " 5   lepton_2_eta              float64\n",
            " 6   lepton_2_phi              float64\n",
            " 7   missing_energy_magnitude  float64\n",
            " 8   missing_energy_phi        float64\n",
            " 9   MET_rel                   float64\n",
            " 10  axial_MET                 float64\n",
            " 11  M_R                       float64\n",
            " 12  M_TR_2                    float64\n",
            " 13  R                         float64\n",
            " 14  MT2                       float64\n",
            " 15  S_R                       float64\n",
            " 16  M_Delta_R                 float64\n",
            " 17  dPhi_r_b                  float64\n",
            " 18  cos(theta_r1)             float64\n",
            " 19  class                     float64\n",
            "dtypes: float64(19), int64(1)\n",
            "memory usage: 534.1 MB\n"
          ]
        }
      ],
      "source": [
        "data_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc8SKpr8IxDe"
      },
      "outputs": [],
      "source": [
        "#remove the unnamed 0 column\n",
        "data_train.drop(columns = ['Unnamed: 0'], inplace = True)\n",
        "data_test.drop(columns = ['Unnamed: 0'], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahbyYkDQJCHV"
      },
      "outputs": [],
      "source": [
        "#splitting the train data into input and output:\n",
        "input = data_train.drop(columns = ['class'])\n",
        "output = data_train['class']\n",
        "\n",
        "cols = input.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV77sJmeKVtW"
      },
      "source": [
        "Next step is standardizing the data. As mentioned above, if we standardize both the test and train dataset together, we are getting higher accuracy. But since that is not a good practice, I am not doing that in the final submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHe73q5nKU7h"
      },
      "outputs": [],
      "source": [
        "#standardizing\n",
        "scaler = StandardScaler()\n",
        "\n",
        "input = scaler.fit_transform(input)\n",
        "data_test = scaler.transform(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKG4E4NULFl-"
      },
      "outputs": [],
      "source": [
        "input = pd.DataFrame(input)\n",
        "input.columns = cols\n",
        "\n",
        "data_test = pd.DataFrame(data_test)\n",
        "data_test.columns = cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TisFDnjKR-zz"
      },
      "source": [
        "Other than the standardization, any other EDA/modification is only resulting in accuracy going down! This maybe due to the fact that the data is already well cleaned. Even if we remove the columns that are highly correlated accuracy goes down. Also finally since the last 10 columns are derived features of the first 8 columns I even tried to use just the last 10 columns but the accuracy was slightly lower (just by 0.5%). In practical situation I would definitely just use the last 10 columns since it was giving comparable accuracy and the training time was much faster. But just for the sake of competition, Iam using everything now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z3Yhe1zLc-A"
      },
      "source": [
        "We will use the train data itself and split it into train and test to check the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJFs--itNIz1",
        "outputId": "8d265d5b-f78b-4363-c19e-26324ac10b5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0    1899670\n",
              "1.0    1600330\n",
              "Name: class, dtype: int64"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X09pbCgvLTKi"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train , Y_test = train_test_split(input, output, random_state = 42, train_size = 0.7, stratify = output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSNk-b5KNaEW"
      },
      "source": [
        "This is where we select the model and do hyperparameter tuning. As menioned above, I used randomsearchcv to tune the hyperparamters. Since the results are different everytime I am directly using the parameters that worked best here.\n",
        "\n",
        "Also for the models that worked best, It was XGboost classifier and HistgradientboostClassifier compared to every other model. Among these two hist is giving slightly better accuracy score. So for the final submission I am using that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6lo06gXOiU9"
      },
      "source": [
        "Finally during the hyperparmeter tuning, the X_train and X_test from above were used and the CV= 5 was taken for crossvalidation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eqAmQBtPQih"
      },
      "outputs": [],
      "source": [
        "model1 = HistGradientBoostingClassifier(max_iter=400, learning_rate=0.1,random_state=42,min_samples_leaf=100,max_leaf_nodes=35,validation_fraction=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L9bsruiOw9U",
        "outputId": "cc7b1064-5187-477d-9c15-c15dc1920f23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HistGradientBoostingClassifier(max_iter=400, max_leaf_nodes=35,\n",
              "                               min_samples_leaf=100, random_state=42,\n",
              "                               validation_fraction=0.2)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model1.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVIO0vjZO2Tp"
      },
      "outputs": [],
      "source": [
        "Y_pred = model1.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEkjLN9OQIp0",
        "outputId": "abf9cdf8-6009-425b-ba63-543dc950f4e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8027914285714286"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(Y_pred,Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIsnl5KwQbff"
      },
      "outputs": [],
      "source": [
        "model2 = xgb.XGBClassifier(objective= 'binary:logistic',subsample = 1.0, colsample_bytree =1.0,n_jobs=16, tree_method = 'hist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7CC2VpoQq4X",
        "outputId": "7cce4e31-2d30-4696-e30f-0910271af2e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "XGBClassifier(colsample_bytree=1.0, n_jobs=16, subsample=1.0,\n",
              "              tree_method='hist')"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAn9My4yQuj4",
        "outputId": "cadc8f91-2b19-4bb2-8308-f5737998e733"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7971171428571429"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_pred = model2.predict(X_test)\n",
        "accuracy_score(Y_pred,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUTGq6a4Rg5B"
      },
      "source": [
        "As we can see above, among the best models I have trained, HistGradinetBoostClassifier is working the best. So will use that for the final submission on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raIkhmbORA7Z"
      },
      "outputs": [],
      "source": [
        "Y_out = model1.predict(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM5a_Uu8R2KN"
      },
      "outputs": [],
      "source": [
        "#writing the output to output file:\n",
        "output_data = pd.DataFrame(Y_out) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXh2H2imTBi8"
      },
      "outputs": [],
      "source": [
        "output_data.to_csv('/content/output.csv', header=['class'], index_label='Id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "059m9zbWTRIt"
      },
      "source": [
        "The resulting file that was submitted to kaggle is in the output.csv file. And the final accuracy is around 80%\n",
        "\n",
        "\n",
        "Throughout this journey I had few major learnings other than what I already knew, even though I couldnt successfully use them in this particular project:\n",
        "1. Feature transformations : I had learned what tranformations to apply to what kind of features based on the distribution and QQ plot. How this will effect the accuracy of linear models like Logistic regression.\n",
        "2. Outliers : How to look for outliers and how to deal with them. \n",
        "3. How to add noise to the data to make it more generalized\n",
        "4. How to read and understand documentation (earlier I just used Youtube!)\n",
        "\n",
        "Is there anything else we can do other than pure Hyperparameter tuning to get a better result? Throughout the journey I felt may times that this time something will change since I tried a new method or had a new insight but none of them produced any result, so I am just wondering.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "fa676f95b428963fc6a22d0f7ab79d1ad8fae9296cc6459b33242ceffa432c07"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}